Parts list:
GoT fics crawled from https://archiveofourown.org/tags/Game%20of%20Thrones%20(TV)/works
got_crawl.json: crawler
got_export_settings.json: crawler settings (note a crawler also run using RESTful API)
got_all.csv
DI_topicsModel.R: R script for generating figures, including Figures 1 and 2
Fig1_Top_Topics_Kudos_Topics.png: Topics by frequency and kudos, figure
Fig2_Wordcloud_topic_split.png: word clouds and a “split wordcloud” comparing relative contributions of shared terms across two related topics

Intro and Aims: What makes certain narrative descriptions engaging or authoritative, and others perhaps confusing or non-descript?  In this project, I propose to examine a corpus of texts with large-scale semantic content and narrative structure, and discern which discourse features of these texts drive a social network to rank them highly or not so highly.  The basic approach underlying this project is scalable to any text-based social network, including twitter, reddit, stackexchange, yelp, etc., but I am specifically looking for a corpus substrate with narrative structure – something that is chaptered and evolving, but autonomous and semantically coherent – so that one can ask more substantive questions about content and writing style down the line. I chose a body of work that is relatively circumscribed topically, but ridiculously entertaining (ironically or otherwise): fanfiction works for the show Game of Thrones. First, I will describe the preliminary analyses done here, and then I will describe how some of the text features we draw out in this first pass might be used as features in a machine learning classifier of the highest vs. lowest ranked works of fiction.Corpus Data: I built a web crawler that collected certain metadata, including author-provided tags and brief summaries, from works of fiction based on the fictional universe depicted in GRR Martin’s A Song of Ice and Fire and the HBO show Game of Thrones.  Data were crawled from the open-source, non-profit archive for fan fiction Archive of Our Own (named one of Time magazine’s 50 Best Websites of 2013). Work updates range from April 2006 to October 2015. Readers on the site can leave “kudos,” something like “likes” or “favorites”, on works, which serves as our main dependent variable against which we can pit certain features of a text, and which, down the line, we can use to train a classifier to predict high-kudos works from their content.Topics model specs: Using a Structural Topic Model (STM; Roberts, Stewart, Tingley, 2015), I extracted topics from the fictions and asked what relation these topics might have with the fictions’ kudos. The data frame of the STM consists of documents, topics, and terms/words, where a topic is a mixture over words (where each word has a certain probability of belonging to a topic), and a document is a mixture over topics such that a given document comprises multiple topics.  The generative process for fitting the topics model proceeds as follows: (1) Across k enumerated topics, find the document-specific distribution over word mixtures representing each topic relative to a baseline word distribution. (2) For each word in the document, draw the word’s topic assignment based on the document-specific distribution over topics. (3) Given the topic, draw a word from that topic. The model converges when the iterations of word and topic draws in 1-3 cease to change the model’s tuning parameters (estimated using a Variational Expectation-Maximization algorithm). While it is generally necessary to tell the model how many topics it should find, I remained agnostic as to the number of topics (perhaps inadvisable in hindsight, as it generated way too many topics in my opinion); initializing the model using singular value decomposition of the word co-occurrence matrix, I projected this matrix into a lowe dimensional space using t-distributed stochastic neighbor embedding (Van Der Maaten 2014), which basically selects k for you.Figures and Analyses:  As a first pass, I conducted STM over the author-provided tags of fictions, where tags are mostly characters and labeled events from the book/TV series. The data set (got_all.csv) also includes brief summaries, also provided by the author, with richer semantic content than the tag fields, but tag fields provided a more tractable first look.The top panel of Figure 1 plots topics (labeled by top 3 terms in topic) by their frequency in the corpus. It appears that many fanfics feature a topic centering on the Stark patriarch and matriarch (Topic 32: Stark, Catelyn, Ned), while the second most common topic regards the character Sandor Clegane and his relationship with the Starks (Topic 75), and so on.The lower panel of Figure 1 homes in on two of these topics, 75 (~Sandor Clegane) and 44 (~Daenerys Targaryen) by their different distributions across kudos. While both topics are roughly equally likely to feature in lower-kudos works (<300), the Sandor topic is more likely than the Daenerys topic to feature in higher-kudos works.  This plot begins to illustrate how we can parse out the contributions of topics to kudos: that is, which topics engage readers the most.While Figure 1 was concerned with topic visualization, Figure 2 visualizes the term structures within topics. The lower two panels of Figure 2 show generic wordclouds indicating the prominence (frequency) of a word conditional on being in topic 44 (the Daenerys-Jorah Mormont topic, left) or in topic 4 (the Daenerys-Khal Drogo topic, right).  More interesting is the upper panel, which pits these two similar topics against each other for inclusion of key terms (including the high-frequency Daenerys and Targaryen tokens). Relative to Topic 4, Topic 44 emphasizes Jorah Mormont at the expense of most other shared terms, while Topic 4 emphasizes Daenerys more, relative to the other terms, than Topic 44, even though both topics focus on this character. Another way to look at this is that Daenerys features more prominently in Khal Drogo contexts than in Jorah Mormont contexts.Future Analyses and Implications:Using topics as dimensions of interest in a text, we can pass a classifier these topic features, training the classifier on a subset of documents (e.g. author-provided tag fields, summaries, or entire fan fictions) and holding out other documents for test in a cross-validation procedure. I have conducted such an analysis on fMRI data, and I would be fascinated to implement the procedure on text features. These features need not be limited to topics: other measures of a text’s style and complexity include metrics like Flesch-Kincaid Readability (at what reading grade-level do these fictions engage readers most?), amount of time between chapter updates (do readers lose interest after a certain amount of time lapses in the serial narrative they’re reading), and many others. All these can serve as features for a classifier that predicts the kudos of a work. I would like ultimately to compare the performance of such a classifier with other benchmarks, including heuristics like the number of words, “uniqueness” (for each word in a document, how unique is it to that document (note, not topic) compared with other documents), “sumbasic” (rank documents by how many high-frequency words they have relative to the collection of documents), and others.This approach has wide-reaching implications for text analysis. While I have not yet touched on the social network aspects of this data set, this site also has a highly concentrated and topicalized social structure. The fan fiction works and authors attract “followers” who comment on, bookmark, critique, and even write stories for one another, and tracking how these social networks interact with content would be a fascinating way to plumb complex social interactions.